%%%% A summary of basic Markov decision theory.

\input{include/header.tex}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLE

\title{\textbf{Markov Decision Processes}\\ \large{A Reasonably Precise Summary}\vspace{-6ex}}
\author{\vspace{-5ex}}
\date{\small{Jason Nezvadovitz -- February 2020}}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BODY

The purpose of this document is to provide a succinct, mathematically precise summary of basic Markov decision theory for review. The focus is on axiomatic clarity rather than intuition. The reader should already have a good understanding of probability theory and basic machine learning.

\vspace{0.2in}
\hrule
\vspace{0.2in}

Declare ``time'': a countable set $\mathbb{T} \coloneqq \{0, 1, 2, \cdots, n\}$ for some potentially infinite ``horizon'' $n$.\\
Denote a generic element as $t \in \mathbb{T}$.\\

In the most general setting, time can be continuous (uncountable), but we will restrict ourselves to discrete time here to avoid the complications of stochastic calculus.\\

Declare the ``state space'': a set $\mathbb{S}$ of any cardinality. Could be $\mathbb{R}^2$, \{\Smiley,\Cooley,\Winkey\}, whatever.\\
Denote a generic element as $s \in \mathbb{S}$.\\

Declare the ``action space'': a set $\mathbb{A}$ of any cardinality. No restrictions.\\
Denote a generic element as $a \in \mathbb{A}$.\\

We will now build a standard \href{https://en.wikipedia.org/wiki/Probability_space}{probability space}, $\boldsymbol{(}\Omega, \mathcal{F}, P\boldsymbol{)}$.\\

Let the sample space be \ $\Omega \coloneqq \mathbb{S}^{n+1} \times \mathbb{A}^n \times \mathbb{R}^n$.\\
An individual sample $\omega \in \Omega$ is called a ``trajectory.''\\

Each trajectory consists of an ``initial condition'' $s_0$, and $n$ triplets of the form $(a_t, s_{t+1}, r_{t+1})$ called ``transitions,'' where the $r \in \mathbb{R}$ are called ``rewards.'' I.e.,
\begin{equation*}
\omega \coloneqq \{s_0, (a_0, s_1, r_1), (a_1, s_2, r_2), \cdots, (a_{n-1}, s_{n}, r_n)\}
\end{equation*}

The event set $\mathcal{F}$ is, as usual, the product sigma-algebra of $\Omega$, using the Borel sigma-algebra for uncountable sets and the power-set sigma-algebra for countable sets.\\

The probability measure $P$ is, as usual, defined in terms of the probability distribution $p$ by sums,
\begin{equation*}
P(B) \coloneqq \int_B p(\omega)\, d\omega\ \ \forall B \in \mathcal{F},\ \ \text{where}\ \ p(\omega)\geq0\ \ \text{and}\ \ \int_\Omega p(\omega)\,d\omega = 1
\end{equation*}

We declare $p$ to have ``Markovian'' structure, i.e. the following factorization:
\begin{align*}
p(\omega) &= p(s_0, a_0, s_1, r_1, a_1, s_2, r_2, \cdots, a_{n-1}, s_{n}, r_n)\\
&\stackrel{\text{markov}}{\coloneqq}\ p(s_0) \prod_{t=0}^{n-1} \red{p(a_t|s_t)}\blue{p(s_{t+1}|s_t,a_t)}\green{p(r_{t+1}|s_t,a_t,s_{t+1})}
\end{align*}

where we are using the usual overloaded notation for marginal and conditional distributions,\\
\begin{equation*}
p(x) \coloneqq \int p(x,y)\,dy\ \ \ \text{and}\ \ \ p(x|y) \coloneqq \frac{p(x,y)}{p(y)}
\end{equation*}

Practically speaking, the Markov property allows us to completely define high-dimensional trajectory probability densities $p(\omega)$ by specifying just the following lower-dimensional distributions:
\begin{itemize}
  \item The ``initial condition'' $p(s_0)$
  \item The ``\red{policies}'' $p(a_t|s_t)\ \forall t$
  \item The ``\blue{dynamics}'' $p(s_{t+1}|s_t,a_t)\ \forall t$
  \item The ``\green{rewards}'' $p(r_{t+1}|s_t,a_t,s_{t+1})\ \forall t$
\end{itemize}

If, for example, the expression of $p(a_t|s_t)$ does not depend explicitly on the time index $t$, then we say there is just one ``policy'' (nonplural) and that it is ``time invariant.'' It is often the case that all of the above are time invariant, and so we really only have one unique policy, one unique dynamic, and one unique reward distribution that are just replicated $n$-times in the factorization of $p(\omega)$.\\

Also, whenever a distribution's form is that of a Dirac/Kronecker-delta, we call it ``deterministic'' instead of ``stochastic.'' It is common for both the policies and rewards to be deterministic. In fact, the reward distribution is often deterministic, time invariant, and even independent of $s_{t+1}$, meaning that it can be fully specified by a single function $\bar{r}:\mathbb{S}\times\mathbb{A} \to \mathbb{R}$ through,
\begin{equation*}
p(r_{t+1}|s_t,a_t,s_{t+1}) = \delta\big{(}r_{t+1} - \bar{r}(s_t, a_t)\big{)} \small{\tag{deterministic reward}}
\end{equation*}

Anyway, the Markov property implies many strong conditional independence statements. E.g.,
\begin{equation*}
p(s_{t+1}|s_t,a_t,s_{t-1},a_{t-1},r_{t-1},s_{t-2},a_{t-2},r_{t-2},\cdots) = p(s_{t+1}|s_t,a_t)
\end{equation*}

These relationships can be depicted succinctly via a \href{https://en.wikipedia.org/wiki/Bayesian_network}{Bayesian graph} over the convenience random variables \ ${s_t}(\omega) \coloneqq s_t$, \ ${a_t}(\omega) \coloneqq a_t$, \ and \ ${r_t}(\omega) \coloneqq r_t$.
\begin{center}
  \includegraphics[width=\linewidth-1.5in]{include/bayesian_graph.png}
\end{center}

This temporally ``chained'' structure models the idea that ``given the present, the future is independent of the past'' and constitutes the entirety of a ``Markov decision process'' (MDP). Specifically, if we define the random variable $\omega_t$ to be the elements of a trajectory for times $\tau \geq t$ (``the future''),
\begin{equation*}
\omega_t \coloneqq \{s_t, (a_t, s_{t+1}, r_{t+1}), (a_{t+1}, s_{t+2}, r_{t+2}), \cdots, (a_{n-1}, s_{n}, r_n)\} \ \in \ \Omega_t \coloneqq \mathbb{S}^{n-t+1} \times \mathbb{A}^{n-t} \times \mathbb{R}^{n-t}
\end{equation*}

we see from the Bayesian graph that $\omega_t$ is disconnected from ``the past'' by conditioning on $s_t$,
\begin{equation*}
p(\omega_t|s_t) = \cancel{\frac{p(s_t)}{p(s_t)}}\prod_{\tau=t}^{n-1} p(a_\tau|s_\tau)p(s_{\tau+1}|s_\tau,a_\tau)p(r_{\tau+1}|s_\tau,a_\tau,s_{\tau+1})
\end{equation*}

We now define the ``total (discounted) reward from $t$'' to be a random variable that assesses the goodness of a sample of our MDP starting from time $t$ by summing up the remaining rewards,
\begin{align*}
G_t(\omega_t) &\coloneqq r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \cdots + \gamma^{n-t-1} r_n\\
&= \sum_{\tau=t}^{n-1} \gamma^{\tau-t} r_{\tau+1}
\end{align*}

where $\gamma \in [0,1]$ is called the ``discount factor,'' used both to enforce the convergence of this sum (if $n \to \infty$) and to express that future rewards are less valuable than sooner rewards.\\

Total reward provokes an optimization problem: we want to choose the best possible policy distribution $p(a_t|s_t)$. To clarify, the ``decision'' in ``Markov decision process'' is really deciding the distribution from which actions will be sampled (the policy). If the policy depends only on time (ignoring the actual value of $s_t$) we call it ``open-loop'' or a ``plan,'' while if it depends on $s_t$ in any way we call it ``closed-loop.'' If the policy is deterministic and time invariant, then it can be fully specified by a single function $\mathbb{S} \mapsto \mathbb{A}$ that is interpreted classically as a decision-making rule.\\

We will denote our choice as $\pi(a_t|s_t)$, and to emphasize that we have plugged-in our choice, we will use $\pi$ as a subscript on probability densities and statistics. For example,
\begin{equation*}
p_{\orange{\pi}}(\omega) \coloneqq p(\omega)\Big{|}_{p(a_t|s_t) = \orange{\pi(a_t|s_t)}} = p(s_0) \prod_{t=0}^{n-1} \orange{\pi(a_t|s_t)}p(s_{t+1}|s_t,a_t)p(r_{t+1}|s_t,a_t,s_{t+1})
\end{equation*}

But what does ``best'' mean with respect to a random variable like $G_t$? We choose to optimize a statistic of $G_t$. Namely, a conditional expectation called the ``value function,''
\begin{align*}
V_\pi(s_t) &\coloneqq E_\pi(G_t | s_t)\\
&= \int_{\Omega_t} G_t(\omega_t) p_\pi(\omega_t|s_t)\,d\omega_t\\
&= \int_{\Omega_t} \sum_{\tau=t}^{n-1} \Big{(}\gamma^{\tau-t} r_{\tau+1}\Big{)} \prod_{\tau=t}^{n-1} \Big{(}\pi(a_\tau|s_\tau)p(s_{\tau+1}|s_\tau,a_\tau)p(r_{\tau+1}|s_\tau,a_\tau,s_{\tau+1})\Big{)} \,d\omega_t
\end{align*}

So $V_\pi(s_t)$ is the expected total reward acquired for starting from state $s$ at time $t$ and acting according to policy $\pi$. Our objective then is to find the optimal policy $\pi^*$ that satisfies,
\begin{equation*}
V_{\pi^*}(s_t) \geq V_{\pi}(s_t)\ \forall s_t
\end{equation*}

There is a necessary and sufficient condition for $\pi^*$ known as the ``dynamic programming principle'':
\begin{quote}
``An optimal policy has the property that, whatever the initial state and initial action are, the remaining actions must constitute an optimal policy with regard to the state resulting from the first action.'' (Bellman, 1957, Chapter III.3)
\end{quote}

This follows directly from the Markov property: given that the process is now in state $s_{t+1}$, the future of the process is independent of how it got there (i.e. the transition from $s_t$ using $a_t$). Therefore, if we can predict the optimal outcome from any $s_{t+1}$, then we can determine $\pi^*(a_t|s_t)$ by optimizing just the expected immediate reward plus our prediction for the optimal outcome from the states we might transition to (discounted by $\gamma$ since it's future reward). Said mathematically,
\begin{gather*}
\pi^*(a_t|s_t) = \delta\big{(}a_t\ ;\ \bar{\pi}^*(s_t)\big{)}\\
\bar{\pi}^*(s_t) = \argmax_{a_t} E\big{(}r_{t+1} + \gamma V_{\pi^*}(s_{t+1}) | s_t, a_t\big{)}
\end{gather*}

However, this only gives us $\pi^*$ in terms of $V_{\pi^*}$ which we still don't know. To resolve this, consider the following recursion satisfied by the total reward,
\begin{align*}
G_t &= r_{t+1} + \gamma \big{(}r_{t+2} + \gamma r_{t+3} + \cdots + \gamma^{n-t-2} r_n\big{)}\\
&= r_{t+1} + \gamma G_{t+1}
\end{align*}

and the following properties of expectation for any random variables $x$, $y$, and $z$,
\begin{align*}
E(\alpha x + \beta y) &= \alpha E(x) + \beta E(y)\\
E(x | z) &= E\big{(}E(x | y, z) | z\big{)}
\end{align*}

It follows that any value function also satisfies a recursion,
\begin{align*}
V_\pi(s_t) &= E_\pi(r_{t+1} + \gamma G_{t+1} | s_t)\\
&= E_\pi(r_{t+1}|s_t) + \gamma E_\pi(G_{t+1} | s_t)\\
&= E_\pi(r_{t+1}|s_t) + \gamma E_\pi\big{(}E_\pi(G_{t+1}|s_{t+1},s_t) | s_t\big{)}\\
&= E_\pi(r_{t+1}|s_t) + \gamma E_\pi\big{(}V_{\pi}(s_{t+1}) | s_t\big{)}\\
&= E_\pi\big{(}r_{t+1} + \gamma V_{\pi}(s_{t+1}) | s_t\big{)}
\end{align*}

And, since there is no more reward to be collected after time $n$, we have a boundary condition,
\begin{equation*}
V_{\pi}(s_n) = 0
\end{equation*}

Plugging-in the dynamic programming principle's $\pi^*$, we arrive at the ``Bellman equation'':
\begin{equation*}
V^*(s_t) = \max_{a_t} E\big{(}r_{t+1} + \gamma V^*(s_{t+1}) | s_t, a_t\big{)}
\end{equation*}

with the shorthand $V^* \coloneqq V_{\pi^*}$. If we can solve the Bellman equation for $V^*$, then $\pi^*$ can be reconstructed as the deterministic policy that always selects the $\argmax$ of the right-hand-side. Thus, a solution $V^*$ to the Bellman equation constitutes a solution to our MDP.\\

It is worth noting that the general value function recursion is useful for more than just deriving the Bellman equation. It lets us compute the value function associated with any policy, which can be useful when comparing candidate policies if the Bellman equation is too difficult to solve directly. The general value function recursion doesn't involve the difficult maximization present in the Bellman equation and is thus linear in the unknown $V_\pi$.\\

The expectations in these equations pack away a lot of computation, though they're not as intense as the expectation in the direct expression of $E_\pi(G_t|s_t)$, because the arguments of these expectations depend only on a single transition, not a whole trajectory, so we only need,
\begin{equation*}
p_\pi(a_t, s_{t+1}, r_{t+1}|s_t) = \orange{\pi(a_t|s_t)} \blue{p(s_{t+1}|s_t,a_t)} \green{p(r_{t+1}|s_t,a_t,s_{t+1})}
\end{equation*}

to express the general value function recursion as,
\begin{align*}
V_\pi(s_t) &= E_\pi\big{(}r_{t+1} + \gamma V_{\pi}(s_{t+1}) | s_t\big{)}\\
&= \orange{\int_{\mathbb{A}} \pi(a_t|s_t) \blue{\int_{\mathbb{S}} p(s_{t+1}|s_t,a_t) \Big{(}\green{\int_{\mathbb{R}}p(r_{t+1}|s_t,a_t,s_{t+1}) r_{t+1} \,dr_{t+1}} + \gamma V_{\pi}(s_{t+1}) \Big{)}\,ds_{t+1}}\,da_t}
\end{align*}

and similarly express the Bellman equation as,
\begin{align*}
V^*(s_t) &= \max_{a_t} E\big{(}r_{t+1} + \gamma V^*(s_{t+1}) | s_t, a_t\big{)}\\
&= \orange{\max_{a_t} \blue{\int_{\mathbb{S}} p(s_{t+1}|s_t,a_t) \Big{(}\green{\int_{\mathbb{R}} p(r_{t+1}|s_t,a_t,s_{t+1})r_{t+1} \,dr_{t+1}} + \gamma V^*(s_{t+1}) \Big{)}\,ds_{t+1}}}
\end{align*}

This is the most general case for a (vanilla) MDP, but there are many very common situations that simplify things. For example, if the reward is deterministic and independent of $s_{t+1}$, it can be pulled out of the $\mathbb{R}$ and $\mathbb{S}$ integrations to write the Bellman equation as,
\begin{equation*}
V^*(s_t) = \max_{a_t} \big{(} \bar{r}(s_t, a_t) + \gamma \mathcal{T}_{a_t} V^*(s_t) \big{)} \small{\tag{deterministic reward}}
\end{equation*}

with the linear ``transition operator'' defined as,
\begin{align*}
\mathcal{T}_{a_t} f(s_t) &\coloneqq E\big{(}f(s_{t+1})|s_t,a_t\big{)}\ \ \ \ \forall f : \mathbb{S} \times \mathbb{T} \to \mathbb{R}\\
&= \int_{\mathbb{S}}p(s_{t+1}|s_t,a_t)f(s_{t+1})\,ds_{t+1}
\end{align*}

Furthermore, if the MDP is time invariant, infinite horizon, and has finite state and action spaces, the above reduces to the form that is often presented first in introductory texts,
\begin{equation*}
V^*(s) = \max_{a} \Big{(} \bar{r}(s, a) + \gamma \sum_{s' \in \mathbb{S}} p(s'|s,a) V^*(s') \Big{)} \small{\tag{deterministic reward, time invariant, finite}}
\end{equation*}

which can be readily implemented as matrix operations on the value function as a single finite vector. It is also worth noting that if the dynamics are deterministic, the sum on the right reduces to a simple evaluation of $V^*$ at the state $s'$ deterministically transitioned to from $s$ using $a$,
\begin{equation*}
V^*(s) = \max_{a} \Big{(} \bar{r}(s, a) + \gamma V^*\big{(}\bar{s}'(s, a)\big{)} \Big{)} \small{\tag{fully deterministic, time invariant, finite}}
\end{equation*}

In general, the Bellman equation is just a fixed-point equation for the nonlinear ``Bellman operator,''
\begin{gather*}
V^*(s_t) = \mathcal{B}V^*(s_t)\\
\mathcal{B} f(s_t) \coloneqq \max_{a_t} E\big{(}r_{t+1} + \gamma f(s_{t+1}) | s_t, a_t\big{)}\ \ \ \ \forall f : \mathbb{S} \times \mathbb{T} \to \mathbb{R}
\end{gather*}

It can be shown that \href{https://people.eecs.berkeley.edu/~ananth/223Spr07/ee223_spr07_lec19.pdf}{the Bellman operator is a contraction mapping}. Thus a variety of fixed-point-iteration-type algorithms can be used to numerically solve the Bellman equation with guaranteed convergence. In ``value iteration,'' the value of $\mathcal{B}\hat{V}^*(s_t)$ is simply used as the next iterate for $\hat{V}^*(s_t)$.
\begin{align*}
&\forall s_t \in {\mathbb{S} \times \mathbb{T}}\ :\\
&\text{Guess}\ \ \hat{V}^*_0(s_t)\\
&\text{Recurse for $i \in \mathbb{N}$\ :}\ \ \ \hat{V}^*_{i+1}(s_t) = \mathcal{B}\hat{V}^*_i(s_t)
\end{align*}

In ``policy iteration,'' the argument of optimization within $\mathcal{B}\hat{V}^*(s_t)$ is used as the next iterate for $\hat{\bar{\pi}}^*(s_t)$, and $\hat{V}^*(s_t)$ is set to the solution of the general value function recursion for $\hat{\bar{\pi}}^*(s_t)$ (a linear subproblem). Many more methods can be conjured-up when just viewing $V^*$ as an $|\mathbb{S}||\mathbb{T}|$-dimensional vector and the Bellman equation as a nonlinear root-finding problem, albeit containing some integrals and a maximization over $\mathbb{A}$ per iteration.\\

The time-complexity is dominated by the number of possible transitions, $|\mathbb{S}||\mathbb{A}||\mathbb{S}'||\mathbb{T}|$, substantially less than the number of possible trajectories $|\Omega|$. The reason for the prime on the second $\mathbb{S}$ is because the set of states it is possible to transition to from a given state and action is often sparse so that $|\mathbb{S}'| \leq |\mathbb{S}|$. For example, if the MDP is deterministic, $|\mathbb{S}'|$ is $1$. Lastly, if the MDP is time invariant, the $|\mathbb{T}|$ can be dropped because the solution for one $t$ will be the same as for any other $t$.\\

For any method, despite convergence, the accuracy of the solution depends on its representation. If the value function is stored entirely (one value for every $s_t$), then the solution will be exact, but this can be intractable for very large state spaces. Furthermore, maximization over the action space can also be intractable for very large action spaces.\\

Large state and/or action spaces are handled by approximating the solution ($V^*$ and/or $\pi^*$) with finitely parameterized function approximators. If the true solution is very smooth or periodic, then few parameters will be needed to exactly capture its behavior. Thus the difficulty in solving an MDP is not really about how large the state and action spaces are, but rather about how regular the underlying $V^*$ is. For example, the well-studied ``linear-quadratic-Gaussian'' (LQG) class of problems has infinite cardinality state and action vector spaces, but the structure of $V^*$ turns out to be a quadratic form and is thus easy to express and solve for exactly.\\

Suppose a real-valued function $\eta(s_t;\theta)$ parameterized by $\theta \in \mathbb{R}^m$ is used to represent our estimate of the optimal value function $\hat{V}^*(s_t)$. We need a mechanism for updating $\theta$ when a new value of $\hat{V}^*(s_t)$ is decided on by value iteration or the like. The most common method is gradient descent on a quadratic error that measures the deviation of the representation from its target value $y \in \mathbb{R}$,
\begin{align*}
\theta_{i+1} &= \theta_i - \alpha \nabla_{\theta}\Big{(}\dfrac{1}{2}\big{(}y - \eta(s_t;\theta)\big{)}^2\Big{)}\big{|}_{\theta=\theta_i}\\
&= \theta_i + \alpha \big{(}y - \eta(s_t;\theta_i)\big{)}\nabla_{\theta}\eta(s_t;\theta_i)%\big{|}_{\theta=\theta_i}
\end{align*}

Using value iteration, the target is simply $y = \mathcal{B}\eta(s_t;\theta_i)$. The fact that the target itself depends on $\theta_i$ is sometimes called the ``moving target'' issue and is what makes ``training'' an approximate solution to the Bellman equation more difficult than supervised learning. The various regions of the state space have to fight for the attention of the relatively small set of parameters, but at the same time they aren't even sure of exactly what they want. Nonetheless, this converges in practice.\\

Up to this point, we have assumed that the ``model'' probabilities of our MDP (the dynamic and reward distributions) were known functions, so that we can compute the necessary expectations. Unfortunately, this is not always the case. In many applications, these probabilities are only known by the ``environment'' (or perhaps a ``simulator'') that can return sample transitions. Specifically, we can query the environment with a particular state and action and it will return a sample next-state and reward from the distribution $p(s_{t+1}, r_{t+1}|s_t,a_t)$. When this is the case, we enter the realm of ``reinforcement learning'' (RL) because we will have to learn the MDP solution by trial-and-error with the actual environment. In RL we may refer to ourselves as the ``agent.''\\

There are two distinct approaches to the RL problem: ``model-based'' and ``model-free.'' In model-based RL, the environment is queried to collect data for forming an approximation of the underlying dynamic and reward distributions. Then, the approximate model is used as if it were exact and the MDP is solved with a technique like the ones covered so far. In contrast, model-free RL uses the environment directly to compute monte-carlo estimates of the expectations in the Bellman equation. We'll cover model-free to avoid a deep discussion of modeling.\\

Recall the Bellman equation in terms of the transition probability distribution,
\begin{align*}
V^*(s_t) &= \mathcal{B}V^*(s_t)\\
&= \max_{a_t} E\big{(}r_{t+1} + \gamma V^*(s_{t+1}) | s_t, a_t\big{)}\\
&= \max_{a_t} \int_{\mathbb{S}}\int_{\mathbb{R}}p(s_{t+1},r_{t+1}|s_t,a_t)\big{(}r_{t+1} + \gamma V^*(s_{t+1})\big{)}\,dr_{t+1}ds_{t+1}
\end{align*}

Let $W(s_t, a_t)$ be a set of sample transitions $(s_{t+1}, r_{t+1})$ obtained from the environment all starting from state $s_t$ and using action $a_t$. A monte-carlo estimate of the Bellman operator is then,
\begin{equation*}
\mathcal{B}V^*(s_t) \approx \max_{a_t} \frac{1}{|W(s_t,a_t)|} \sum_{W(s_t,a_t)} \big{(}r_{t+1} + \gamma V^*(s_{t+1})\big{)}
\end{equation*}

To build $W$ we would need to iterate over every possible $(s_t, a_t)$ pair and query a sufficient number of transition samples. This is a lot of work, and a lot of data. How many samples is sufficient anyway? In the end, the optimal policy may not even have us go near certain regions of the state space, so maybe those areas don't need to be estimated as well as other more important areas.\\

Furthermore, sometimes our environment is not nice enough to let us query for any starting state at will. For example, a compiled video-game wont just let us set the game state. It instead forces us to play from the beginning every trial, so the samples we get are just pieces of trajectories that occurred based on the actions our policy selected. But this is circular, since our policy needs the samples already for it to be computed as the $\argmax$ of the estimated expectation.\\

To resolve this, we'll rewrite the Bellman equation into a form that allows for the selection of $a_t$ without evaluating an expectation. Define the optimal ``action-value function'' $Q^*:\mathbb{S}\times\mathbb{A}\times\mathbb{T} \to \mathbb{R}$ as the expected total reward for starting at state $s_t$, using action $a_t$, and from then on acting according to the optimal policy,
\begin{align*}
Q^*(s_t, a_t) &\coloneqq E_{\pi^*}(G_t | s_t, a_t)\\
&= E\big{(}r_{t+1} + \gamma V^*(s_{t+1}) | s_t, a_t\big{)}
\end{align*}

Then we can compute the optimal policy as,
\begin{equation*}
\bar{\pi}^*(s_t) = \argmax_{a_t} Q^*(s_t, a_t)
\end{equation*}

All we have done of course is hide the expectation, but from an algorithmic perspective this makes a conceptual difference. To choose actions, instead of maximizing the expectation of an estimate of $V^*$, we can just maximize an estimate of $Q^*$. Rewriting the Bellman equation in terms of $Q^*$,
\begin{gather*}
V^*(s_t) = \max_{a_t} Q^*(s_t, a_t)\\
\implies\ \ \ Q^*(s_t, a_t) = E\big{(}r_{t+1} + \gamma \max_{a_{t+1}} Q^*(s_{t+1}, a_{t+1}) | s_t, a_t\big{)}
\end{gather*}

So by maximizing our current estimate of $Q^*$ we can select actions to interact with the environment, and each transition sample we get from an interaction can be used to monte-carlo estimate a fixed-point-iteration to refine our $Q^*$ estimate. This bootstrap method is known as ``Q-learning.''
\begin{align*}
&\text{Guess}\ \ \hat{Q}^*_0\\
&\text{For each ``epoch'' :}\\
&\text{Sample}\ \ s_0\ \dist\ p(s_0)\\
&\text{For each $t \in \mathbb{T}$ :}\\
&\text{Select}\ \ u_t = \argmax_{a_t} \hat{Q}^*_i(s_t, a_t)\\
&\text{Sample}\ \ s_{t+1}, r_{t+1}\ \dist\ p{(}s_{t+1}, r_{t+1} | s_t, a_t=u_t{)}\\
&\text{Recurse}\ \ \hat{Q}^*_{i+1}{(}s_t, u_t{)} = r_{t+1} + \gamma \max_{a_{t+1}} \hat{Q}^*_i{(}s_{t+1}, a_{t+1}{)}
\end{align*}

And this can of course be combined with function approximation as needed. To help promote exploration / avoid local optima, sometimes an ``$\epsilon$-greedy'' policy is used, where a random action is selected for $u_t$ with probability $\epsilon$ and the approximately optimal action (as shown above) is selected with probability $1-\epsilon$. The $Q^*$ recursion's maximization means it will still learn the optimal $Q$ regardless of what policy is actually used to interact with the environment. Thus Q-learning is deemed an ``off-policy'' method. A slight variant called ``SARSA'' is an ``on-policy'' method where the recursion simply uses $\hat{Q}^*_i{(}s_{t+1}, u_{t+1}{)}$ for the rightmost term. Lastly, to help reduce the variance of these single-sample monte-carlo estimates and assist convergence, the sampled transitions are often recorded in a database called ``experience,'' randomly ordered, and then reused for the $\hat{Q}$ update, a technique called ``experience replay.''\\

The Bellman equation can be understood as using the Markov property to ``roll'' the MDP objective $E_{\pi^*}(G_t | s_t)$ into a recursion over a single transition. The switch to $Q^*$ can be understood as ``unrolling'' just the first step, so $Q$ might be called a ``1-step value function'' and $V$ a ``0-step value function.'' If the MDP is unrolled out to $n$ steps, then we arrive back at the original optimization over trajectories rather than transitions, i.e. no use of the dynamic programming principle.\\

Some approaches tackle the MDP this way, forming a monte-carlo approximation of the gradient of the entire $E_{\pi^*}(G_t | s_t)$ ``roll-out'' and using it to improve a parameterized policy distribution. These ``direct policy gradient'' methods are way less sample efficient than dynamic programming, but do not require global maximizations over $\mathbb{A}$. Policy gradient methods have become popular because even local optima can make for very effective policies in practice.\\

The dynamic programming methods can even be combined with policy gradient methods, in what are called ``actor-critic'' methods. The policy function approximator is the ``actor'' while the value function approximator is the ``critic.'' The purpose is to reduce the variance in the policy gradient estimates by honing a predictor of expected optimal value that can be used as a ``baseline.''\\

To finish, lets allude to a few more places MDP theory can take us. So far, we have assumed that the current state was always available to choose an action based on. But what if the MDP had an observation model $p(z_t|s_t,a_t)$ and our policy was restricted to being a function of only the observations? This is called a ``partially observed MDP'' (POMDP) or ``hidden Markov model'' (HMM). It elicits the idea of ``state estimation'' design.\\

Furthermore, our MDP seems to model a universe where there is only one agent choosing actions. Many situations have multiple agents competing with each other. Entering the realm of ``game theory,'' we can study ``minimax''-type problems and game theoretic versions of dynamic programming theory that handle how an agent should choose its actions knowing that the environment contains an adversary agent with its own objective in mind.\\

Fascinating indeed! Check it out!\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
